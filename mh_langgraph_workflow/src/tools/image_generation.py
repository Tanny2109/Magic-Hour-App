"""Image generation tool using fal.ai and Prodia APIs."""
import os
import json
import time
import tempfile
from io import BytesIO
from typing import Optional, Literal
from concurrent.futures import ThreadPoolExecutor, as_completed

import fal_client
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from pydantic import BaseModel, Field
from langchain_core.tools import tool
from PIL import Image

# Configure fal.ai
fal_client.api_key = os.getenv("FAL_KEY")

# Prodia session setup (matching smolagents)
prodia_token = os.getenv('PRODIA_KEY')
prodia_url = 'https://inference.prodia.com/v2/job'

prodia_session = requests.Session()
retries = Retry(allowed_methods=None, status_forcelist=Retry.RETRY_AFTER_STATUS_CODES)
prodia_session.mount('http://', HTTPAdapter(max_retries=retries))
prodia_session.mount('https://', HTTPAdapter(max_retries=retries))
prodia_session.headers.update({'Authorization': f"Bearer {prodia_token}"})

prodia_headers = {
    'Accept': "image/jpeg",
    'Content-Type': 'application/json',
}

# Model mapping for pro mode
PRO_MODELS = {
    "seedream4": "fal-ai/bytedance/seedream/v4/text-to-image",
    "imagen4_ultra": "fal-ai/imagen4/preview/ultra",
    "flux-krea": "fal-ai/flux/krea",
}


class ImageGenerationInput(BaseModel):
    """Input schema for image generation."""
    prompt: str = Field(description="Detailed text description of the image to generate")
    mode: Literal["fast", "pro"] = Field(
        default="fast",
        description="Generation mode: 'fast' uses Prodia for quick results, 'pro' uses fal.ai for higher quality"
    )
    model: str = Field(
        default="flux-krea",
        description="Model for pro mode: 'seedream4', 'imagen4_ultra', 'flux-krea'. Default: flux-krea"
    )
    width: int = Field(default=720, description="Image width in pixels (256-2048)")
    height: int = Field(default=720, description="Image height in pixels (256-2048)")
    num_images: int = Field(default=1, description="Number of images to generate (1-4)")
    negative_prompt: Optional[str] = Field(
        default=None,
        description="Things to avoid in the generated image"
    )


def _generate_single_image_prodia(prompt: str, width: int, height: int) -> str:
    """Generate a single image using Prodia API (fast mode) - matches smolagents."""
    job = {
        "type": "inference.flux-fast.schnell.txt2img.v2",
        "config": {
            "prompt": prompt,
            "width": width,
            "height": height,
            "steps": 4
        }
    }
    res = prodia_session.post(prodia_url, headers=prodia_headers, json=job)
    res.raise_for_status()
    img = Image.open(BytesIO(res.content))
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
    img.save(temp_file.name)
    return temp_file.name


def _generate_single_image_fal(prompt: str, model_id: str, width: int, height: int) -> str:
    """Generate a single image using fal.ai API (pro mode)."""
    args = {
        "prompt": prompt,
        "image_size": {"width": width, "height": height},
        "num_images": 1,
    }

    handler = fal_client.submit(model_id, arguments=args)
    result = handler.get()

    if "images" in result and result["images"]:
        img_url = result["images"][0].get("url")
        if img_url:
            response = requests.get(img_url)
            img = Image.open(BytesIO(response.content))
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
            img.save(temp_file.name)
            return temp_file.name

    raise ValueError("No image generated by fal.ai")


@tool(args_schema=ImageGenerationInput)
def generate_images(
    prompt: str,
    mode: Literal["fast", "pro"] = "fast",
    model: str = "flux-krea",
    width: int = 720,
    height: int = 720,
    num_images: int = 1,
    negative_prompt: Optional[str] = None
) -> str:
    """Generate images from a text prompt using AI models.

    Use this tool when the user wants to create new images from scratch based on a description.
    Returns file paths to the generated images.

    Args:
        prompt: Detailed text description of the image to generate
        mode: 'fast' for quick generation (Prodia), 'pro' for higher quality (fal.ai)
        model: Model for pro mode - 'seedream4', 'imagen4_ultra', 'flux-krea'
        width: Image width in pixels
        height: Image height in pixels
        num_images: Number of images to generate (1-4)
        negative_prompt: Things to avoid in the generated image

    Returns:
        String containing the file paths of generated images, separated by newlines
    """
    # Clamp values
    width = max(256, min(2048, width))
    height = max(256, min(2048, height))
    num_images = max(1, min(4, num_images))

    # Get model ID for pro mode
    model_id = PRO_MODELS.get(model, PRO_MODELS["flux-krea"])

    # Debug logging
    debug_info = {
        "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
        "mode": mode,
        "model": model if mode == "pro" else "prodia-flux-schnell",
        "model_id": model_id if mode == "pro" else "inference.flux-fast.schnell.txt2img.v2",
        "width": width,
        "height": height,
        "num_images": num_images,
    }
    print(f"\n{'='*80}")
    print(f"DEBUG - Image Generation Tool Called:")
    print(json.dumps(debug_info, indent=2))
    print(f"{'='*80}\n")

    st = time.time()
    generated_paths = []
    errors = []

    if mode == "fast":
        # Fast mode: Use Prodia API with parallel generation
        def generate_single(idx):
            return _generate_single_image_prodia(prompt, width, height)

        with ThreadPoolExecutor(max_workers=num_images) as executor:
            future_list = [executor.submit(generate_single, i) for i in range(num_images)]
            for future in as_completed(future_list):
                try:
                    generated_paths.append(future.result())
                except Exception as e:
                    print(f"DEBUG - Error generating image: {e}")
                    errors.append(str(e))
    else:
        # Pro mode: Use fal.ai with parallel generation
        def generate_single(idx):
            return _generate_single_image_fal(prompt, model_id, width, height)

        with ThreadPoolExecutor(max_workers=min(num_images, 4)) as executor:
            future_list = [executor.submit(generate_single, i) for i in range(num_images)]
            for future in as_completed(future_list):
                try:
                    result_path = future.result()
                    if result_path:
                        generated_paths.append(result_path)
                except Exception as e:
                    print(f"DEBUG - Error generating image: {e}")
                    errors.append(str(e))

    et = time.time()

    if not generated_paths:
        print(f"DEBUG - Image generation FAILED after {et - st:.2f}s: {errors}")
        return f"Failed to generate images: {'; '.join(errors)}"

    print(f"DEBUG - Generated {len(generated_paths)} images in {et - st:.2f}s")
    for path in generated_paths:
        print(f"DEBUG - Image path: {path}")

    result = f"Generated {len(generated_paths)} image(s): {', '.join(generated_paths)}"

    if errors:
        result += f"\nSome generations failed: {'; '.join(errors)}"

    print(f"DEBUG - Image generation result:\n{result}")
    return result
